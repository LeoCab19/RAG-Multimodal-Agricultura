# Asistente Agr√≠cola RAG Multimodal

Sistema de consulta t√©cnica agr√≠cola con b√∫squeda h√≠brida, re-ranking y generaci√≥n multimodal usando VLM.

## Caracter√≠sticas

- **B√∫squeda H√≠brida**: Vectorial (E5) + BM25 fusionados con RRF
- **Re-ranking**: Cross-encoder para m√°xima precisi√≥n
- **Multimodal**: Vincula im√°genes con texto por proximidad espacial
- **Streaming**: Respuestas en tiempo real con memoria conversacional
- **Evaluaci√≥n**: M√©tricas de Fidelidad, Relevancia y Recall

## Stack

**Modelos**: `intfloat/multilingual-e5-large` (embeddings) ‚Ä¢ `BAAI/bge-reranker-v2-m3` (reranker) ‚Ä¢ `Qwen2-VL-2B` (generaci√≥n)  
**Backend**: FastAPI + ChromaDB + BM25  
**Frontend**: Streamlit

## Instalaci√≥n

```bash
# Dependencias
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118
pip install -r requirements.txt

# Estructura
mkdir -p pdf images chroma_db

# Variables de entorno (.env)
VLM_MODEL=Qwen/Qwen2-VL-2B-Instruct
MODELO_RERANKER=BAAI/bge-reranker-v2-m3
TOP_K=10
MAX_TOKENS_LLM=10000
```

## Uso

### 1. Ingestar PDFs
```bash
# Coloca PDFs en ./pdf/
python chunkingV5ImagenMejorado.py
```
Extrae texto, filtra im√°genes (>100px, brillo 40-220, variaci√≥n >12) y genera embeddings con prefijo `passage:`.

### 2. Iniciar API
```bash
python api_rag_multimodal1.py  # http://127.0.0.1:8000
```

### 3. Ejecutar UI
```bash
streamlit run streamli_front.py  # http://localhost:8501
```

## Arquitectura

```
Usuario ‚Üí Query Expansion ‚Üí [B√∫squeda Vectorial + BM25] ‚Üí RRF ‚Üí 
Re-ranking ‚Üí Mejor Doc + Imagen ‚Üí VLM (streaming) ‚Üí Respuesta
```

**Optimizaciones clave**:
- Cach√© LRU de ChromaDB (`@lru_cache`)
- √çndice BM25 pre-cargado en RAM (startup)
- Prefijos E5: `query:` para b√∫squedas, `passage:` para docs

### Evaluaci√≥n de Recuperaci√≥n (Retrieval)
Para garantizar la precisi√≥n del asistente, se evaluaron tres configuraciones de segmentaci√≥n de documentos (chunking). El objetivo fue maximizar la relevancia del primer resultado recuperado.

Primero se ha tenido que utiliza un script para hacer el chunk peque√±o y el chunk mediano.
Hemos utilizado el siguiente script:
```bash
python preparan_evaluacion.py
```
Despues para el resultado se tiene que ejecutar el siguiente script:

```bash
python evaluador_retrivel.py
```
### Tabla Comparativa de Configuraciones RAG

| Configuraci√≥n | Tokens | Overlap | Hit Rate @3 | MRR @3 | Latencia |
|---------------|--------|---------|-------------|--------|----------|
| **Peque√±o** | 256 | 20 | 1.0 | 1.0 | Baja |
| **Medio** | 512 | 50 | 1.0 | 1.0 | Media |
| **Grande** | 1024 | 100 | 1.0 | 0.5 | Alta |

**Recomendaci√≥n**: Configuraci√≥n **Media** (512 tokens, overlap 50) ofrece el mejor balance precisi√≥n/velocidad.

## Evaluaci√≥n

### Crear Golden Set
```bash
python generar_evaluador.py
```
Genera casos de prueba: pregunta + respuesta esperada + IDs relevantes.

### Ejecutar Tests
```bash
python evaluacion_ragas_api.py
```

**M√©tricas**:
- **Fidelidad** (0-100%): Sin alucinaciones (meta >90%)
- **Relevancia** (1-5): Utilidad pr√°ctica (meta >4.0)
- **Recall** (0-100%): Documentos recuperados (meta >85%)

Ejemplo:
```
‚úÖ Fidelidad:   90.0%
üéØ Relevancia:  1.80/5
üîç Recall:      100.0%
```

##  Configuraci√≥n

### Ajustar Rendimiento
```python
# api_rag_multimodal1.py
TOP_K = 10              # Candidatos (5-20)
temperature = 0.1       # Creatividad (0.05-0.7)
max_new_tokens = 10000  # Longitud m√°xima

# RRF k=60 (balance), k=20 (agresivo), k=100 (conservador)
```

### CPU (sin GPU)
```python
device = "cpu"
torch_dtype = torch.float32  # No float16
TOP_K = 5  # Reducir carga
```
### Tambien se puede utilizar con GPU

## Formato de Datos

**ChromaDB**:
```json
{
  "id": "manual_p23_b5",
  "document": "El pulg√≥n negro...",
  "metadata": {"source": "manual.pdf", "page": 23, "images": "./images/p23.png"},
  "embedding": [...]  // 1024 dims
}
```

**Golden Set** (JSONL):
```json
{"query": "¬øControl de pulg√≥n?", "ground_truth": "Jab√≥n 2%...", "relevant_ids": ["manual_p23_b5"]}
```

## Troubleshooting

| Error | Soluci√≥n |
|-------|----------|
| Collection not found | `python chunkingV5ImagenMejorado.py` |
| CUDA OOM | Reducir `TOP_K=5` o `MAX_TOKENS_LLM=500` |
| API lenta | √çndice BM25 cargado? Revisar logs de cach√© |
| Baja fidelidad | `temperature=0.05`, fortalecer system prompt |
| Bajo recall | Verificar prefijos E5, aumentar `TOP_K=20` |

## Flujo de B√∫squeda H√≠brida

1. **Query Expansion**: `"ara√±a"` ‚Üí `"Informaci√≥n t√©cnica sobre... ara√±a"`
2. **Vectorial**: Embedding query ‚Üí ChromaDB (cosine similarity)
3. **BM25**: Tokenizaci√≥n ‚Üí √çndice invertido (TF-IDF)
4. **RRF**: Fusi√≥n con `score = Œ£[1/(60+rank)]`
5. **Rerank**: Cross-encoder eval√∫a pares [query, doc]
6. **Mejor**: Top-1 + imagen vinculada ‚Üí Prompt VLM




