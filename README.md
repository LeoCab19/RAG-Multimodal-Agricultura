# ğŸŒ¾ Asistente AgrÃ­cola RAG Multimodal

Sistema de consulta tÃ©cnica agrÃ­cola con bÃºsqueda hÃ­brida, re-ranking y generaciÃ³n multimodal usando VLM.

## ğŸ¯ CaracterÃ­sticas

- **BÃºsqueda HÃ­brida**: Vectorial (E5) + BM25 fusionados con RRF
- **Re-ranking**: Cross-encoder para mÃ¡xima precisiÃ³n
- **Multimodal**: Vincula imÃ¡genes con texto por proximidad espacial
- **Streaming**: Respuestas en tiempo real con memoria conversacional
- **EvaluaciÃ³n**: MÃ©tricas de Fidelidad, Relevancia y Recall

## ğŸ› ï¸ Stack

**Modelos**: `intfloat/multilingual-e5-large` (embeddings) â€¢ `BAAI/bge-reranker-v2-m3` (reranker) â€¢ `Qwen2-VL-2B` (generaciÃ³n)  
**Backend**: FastAPI + ChromaDB + BM25  
**Frontend**: Streamlit

## ğŸ“¦ InstalaciÃ³n

```bash
# Dependencias
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118
pip install -r requirements.txt

# Estructura
mkdir -p pdf images chroma_db

# Variables de entorno (.env)
VLM_MODEL=Qwen/Qwen2-VL-2B-Instruct
MODELO_RERANKER=BAAI/bge-reranker-v2-m3
TOP_K=10
MAX_TOKENS_LLM=10000
```

## ğŸš€ Uso

### 1. Ingestar PDFs
```bash
# Coloca PDFs en ./pdf/
python chunkingV5ImagenMejorado.py
```
Extrae texto, filtra imÃ¡genes (>100px, brillo 40-220, variaciÃ³n >12) y genera embeddings con prefijo `passage:`.

### 2. Iniciar API
```bash
python api_rag_multimodal1.py  # http://127.0.0.1:8000
```

### 3. Ejecutar UI
```bash
streamlit run streamli_front.py  # http://localhost:8501
```

## âš™ï¸ Arquitectura

```
Usuario â†’ Query Expansion â†’ [BÃºsqueda Vectorial + BM25] â†’ RRF â†’ 
Re-ranking â†’ Mejor Doc + Imagen â†’ VLM (streaming) â†’ Respuesta
```

**Optimizaciones clave**:
- CachÃ© LRU de ChromaDB (`@lru_cache`)
- Ãndice BM25 pre-cargado en RAM (startup)
- Prefijos E5: `query:` para bÃºsquedas, `passage:` para docs

## ğŸ“Š EvaluaciÃ³n

### Crear Golden Set
```bash
python generar_evaluador.py
```
Genera casos de prueba: pregunta + respuesta esperada + IDs relevantes.

### Ejecutar Tests
```bash
python evaluacion_ragas_api.py
```

**MÃ©tricas**:
- **Fidelidad** (0-100%): Sin alucinaciones (meta >90%)
- **Relevancia** (1-5): Utilidad prÃ¡ctica (meta >4.0)
- **Recall** (0-100%): Documentos recuperados (meta >85%)

Ejemplo:
```
âœ… Fidelidad:   90.0%
ğŸ¯ Relevancia:  1.80/5
ğŸ” Recall:      100.0%
```

## ğŸ”§ ConfiguraciÃ³n

### Ajustar Rendimiento
```python
# api_rag_multimodal1.py
TOP_K = 10              # Candidatos (5-20)
temperature = 0.1       # Creatividad (0.05-0.7)
max_new_tokens = 10000  # Longitud mÃ¡xima

# RRF k=60 (balance), k=20 (agresivo), k=100 (conservador)
```

### CPU (sin GPU)
```python
device = "cpu"
torch_dtype = torch.float32  # No float16
TOP_K = 5  # Reducir carga
```
### Tambien se puede utilizar con GPU

## ğŸ“ Formato de Datos

**ChromaDB**:
```json
{
  "id": "manual_p23_b5",
  "document": "El pulgÃ³n negro...",
  "metadata": {"source": "manual.pdf", "page": 23, "images": "./images/p23.png"},
  "embedding": [...]  // 1024 dims
}
```

**Golden Set** (JSONL):
```json
{"query": "Â¿Control de pulgÃ³n?", "ground_truth": "JabÃ³n 2%...", "relevant_ids": ["manual_p23_b5"]}
```

## ğŸ› Troubleshooting

| Error | SoluciÃ³n |
|-------|----------|
| Collection not found | `python chunkingV5ImagenMejorado.py` |
| CUDA OOM | Reducir `TOP_K=5` o `MAX_TOKENS_LLM=500` |
| API lenta | Ãndice BM25 cargado? Revisar logs de cachÃ© |
| Baja fidelidad | `temperature=0.05`, fortalecer system prompt |
| Bajo recall | Verificar prefijos E5, aumentar `TOP_K=20` |

## ğŸ“ˆ Flujo de BÃºsqueda HÃ­brida

1. **Query Expansion**: `"araÃ±a"` â†’ `"InformaciÃ³n tÃ©cnica sobre... araÃ±a"`
2. **Vectorial**: Embedding query â†’ ChromaDB (cosine similarity)
3. **BM25**: TokenizaciÃ³n â†’ Ãndice invertido (TF-IDF)
4. **RRF**: FusiÃ³n con `score = Î£[1/(60+rank)]`
5. **Rerank**: Cross-encoder evalÃºa pares [query, doc]
6. **Mejor**: Top-1 + imagen vinculada â†’ Prompt VLM




